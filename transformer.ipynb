{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pass\n",
    "\n",
    "# class FlashAttention(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, q, k, v, mask=None):\n",
    "#         pass\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pass\n",
    "\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         pass\n",
    "\n",
    "# class RotaryPositionalEncoding(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         pass\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "#         pass\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        pass\n",
    "\n",
    "def create_causal_mask():\n",
    "    pass\n",
    "\n",
    "def initialize_weights():\n",
    "    pass\n",
    "\n",
    "def load_model():\n",
    "    pass\n",
    "\n",
    "def save_model():\n",
    "    pass\n",
    "\n",
    "def generate_text():\n",
    "    pass\n",
    "\n",
    "def train_step():\n",
    "    pass\n",
    "\n",
    "def evaluate():\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_lm.hf import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TRI-ML/DCLM-1B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TRI-ML/DCLM-1B\")\n",
    "\n",
    "inputs = tokenizer([\"Machine learning is\"], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from threading import local\n",
    "\n",
    "\n",
    "def flash_attention(q,k,v,mask=None,dropout=None,block_size=1024):\n",
    "    #extractttt info from query tensor\n",
    "    batch_size,num_heads,seq_len,d_k=q.size()\n",
    "    #scaling factor for the dot product attention\n",
    "    scale=1/math.sqrt(d_k)\n",
    "    #intializee tensors to store final output and atteention weights\n",
    "    output=torch.zeros_like(q)\n",
    "    attention_weights=torch.zeros(batch_size,seq_len,seq_len,device=q.device)\n",
    "\n",
    "    #itterate over the sequence in block s\n",
    "    for block_start in range(0,seq_len,block_size):\n",
    "        #caclulate the end of the block\n",
    "        block_end=min(block_start+block_size,seq_len)\n",
    "        \n",
    "        #extracl block for query key and values \n",
    "        local_q=q[::,:,block_start:block_end]\n",
    "        local_k=k[:,:,:block_end]\n",
    "        local_v=v[:,:,:block_end]\n",
    "\n",
    "        #local attention scaled dod product fo it is calcualted\n",
    "        local_attention=torch.matmul(local_q,local_k.transpose(-1,-2))*scale\n",
    "\n",
    "        #apply mask if provided\n",
    "        if mask is not None:\n",
    "            local_attn=local_attn+mask[:,:,block_start:block_end,:block_end]\n",
    "\n",
    "        #apply softmax\n",
    "        local_attn=F.softmax(local_attn,dim=-1)\n",
    "\n",
    "        #apply dropout if provided \n",
    "        if dropout is not None:\n",
    "            local_attn=dropout(local_attn)\n",
    "\n",
    "        #compute output of current block\n",
    "        local_output=torch.matmul(local_attn,local_v)\n",
    "\n",
    "        #store output of current block\n",
    "        output[:,:,block_start:block_end]=local_output\n",
    "        attention_weights[:,:,block_start:block_end]=local_attn\n",
    "\n",
    "    return output,attention_weights    \n",
    "\n",
    "def flash_scaled_dot_product(q,k,v,mask):\n",
    "    d_k=q.size()[:-1]\n",
    "    values,attention=flash_attention(q,k,v,mask)\n",
    "    return values,attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rotaryembedding(nn.Module):\n",
    "    def __init__(self,dim):\n",
    "        super().__init__()\n",
    "        self.dim=dim\n",
    "    def forward(self,seq_len):\n",
    "        #asmuch as i understand thsi is cuz we add the psostionsal emvbeddings for evene and odd seeperatelty so calcuating it for half the dimentionality\n",
    "        half_dim=self.dim//2\n",
    "        #base for exponential decay \n",
    "        emb=math.log(10000)/(half_dim-1)\n",
    "        #basically wavelenghts that increase exponentially\n",
    "        emb=torch.exp(torch.arange(half_dim,device=self.device)*-emb)\n",
    "        #outer prodcut creates sine waves\n",
    "        emb=torch.outer(torch.arange(seq_len,device=self.device),emb)    \n",
    "        #this creates pairs of orthogonal vector for each psoition\n",
    "        emb=torch.stack([torch.cos(emb),torch.sin(emb)],dim=-1)\n",
    "        #unsqueeze like devides vector to size one dim \n",
    "        return emb.unsqueeze(1)\n",
    "\n",
    "class RotaryPosEnc(nn.Module):\n",
    "    def __init__(self,d_model,max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length=max_sequence_length\n",
    "        self.d_model=d_model\n",
    "        self.rotary_emb=rotaryembedding(self.d_model//2)\n",
    "    def forward(self,x):\n",
    "        pos_emb=self.rotary_emb(self.max_sequence_length)\n",
    "        cos_pos,sin_pos=pos_emb.unbind(dim=-1)\n",
    "        x_even,x_odd=x.chunk(2,dim=-1)\n",
    "        x_even_new=x_even*cos_pos-x_odd-sin_pos\n",
    "        x_odd_new=x_odd*cos_pos-x_even*sin_pos\n",
    "\n",
    "        x_new=torch.stack([x_even_new,x_odd_new],dim=-1)\n",
    "        return x_new.flatten(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoderlayer(nn.Module):\n",
    "    def __init__(self,d_model,num_heads,d_ff,dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn=FlashAttention(d_model,num_heads)\n",
    "        self.norm1=LayerNorm(d_model)\n",
    "        self.feed_forward=FeedForward(d_model,d_ff)\n",
    "        self.norm2=LayerNorm(d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x,mask=None):\n",
    "        attn_output=self.self_attn(x,x,x,mask)\n",
    "        x=self.norm1(x+self.dropout(attn_output))\n",
    "        ff_output=self.feed_forward(x)\n",
    "        x=self.norm2(x+self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,num_heads,num_layers,d_ff,max_sequence_length,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embeddings=nn.Embedding(vocab_size,d_model) \n",
    "        self.rotary_pe=RotaryPositionalEncoding(d_model,max_sequence_length)\n",
    "        self.layers=nn.ModuleList([\n",
    "            Decoderlayer(d_model,num_heads,d_ff,dropout)\n",
    "            for _ in range(num_layers)\n",
    "            ])\n",
    "\n",
    "        self.layer_norm=LayerNorm(d_model)\n",
    "        self.ouput_projections=nn.Linear(d_model,vocab_size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,x,mask=None):\n",
    "        x=self.token_embeddings(x)\n",
    "        x=self.rotary_pe(x)\n",
    "        x=self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "        x=self.layer_norm(x)\n",
    "        logits=self.output_projections(x)\n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LayerNorm(nn.Module):\n",
    "    def __init__(self,d_model,epsilon=1e-5):\n",
    "        super().__init__()\n",
    "        self.gamma=nn.Parameter(torch.ones(d_model))\n",
    "        # Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.\n",
    "        self.beta=nn.parameter(torch.zeros(d_model))\n",
    "        self.epsilon=epsilon\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        var=x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        x_norm=(x-mean)/torch.sqrt(var+self.epsilon)\n",
    "        #these learnable parameters adjust themselves to mayeb undo the normalization if needed \n",
    "        out=self.gamma*x_norm+self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ff.dropout=0.1,activation='gelu'):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.dropout=dropout\n",
    "        self.linear1=nn.Linear(d_model,d_ff)\n",
    "        self.linear2=nn.Linear(d_ff,d_model)\n",
    "        self.dropout_layer=nn.Dropout(dropout)\n",
    "        self.activation=F.gelu\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.activation(x)\n",
    "        x=self.dropout_layer(x)\n",
    "        x=self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model,num_heads,num_layers,d_ff,max_seq_leangth,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding=nn.Embedding(vocab_size,d_model)\n",
    "        self.rotary_pe=RotaryPosEnc(d_model,max_seq_leangth)\n",
    "        self.decoder=Decoder(d_model=d_model,vocab_size=vocab_size,max_sequence_length=max_seq_leangth,num_heads=num_heads,num_layers=num_layers,d_ff=d_ff,dropout=dropout)\n",
    "        self.final_layer_norm=LayerNorm(d_model)\n",
    "        self.ouput_projection=nn.Linear(d_model,vocab_size)\n",
    "\n",
    "    def forward(self,input_ids,attention_mask=None):\n",
    "        x=self.token_embeding(input_ids)\n",
    "        x=self.rotary_pe(x)\n",
    "        x=self.decoder(x,mask=attention_mask)\n",
    "        x=self.final_layer_norm(x)\n",
    "        logits=self.ouput_projection(x)\n",
    "        return logits   \n",
    "    def generate(self,input_ids,max_length,temperature=1.0):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length-input_ids.size(1)):\n",
    "                outputs=self(input_ids)\n",
    "                next_token_logits=outputs[:,-1,:]/temperature\n",
    "                next_token_probs=torch.softmax(next_token_logits,dim=-1)\n",
    "                next_token=torch.multinomial(next_token_probs,num_samples=1)\n",
    "                input_ids=torch.cat([input_ids,next_token],dim=-1)\n",
    "                if next_token.item()==self.eos_token_id:\n",
    "                    break\n",
    "        return input_ids        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "def save_model(model,path):\n",
    "    torch.save(model.state_dict(),path)\n",
    "def initialize_weights(model):\n",
    "    for p in model.parameters():\n",
    "        if p.dim()>1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "def train_step(model,optimizer,batch,loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    input_ids=batch['input_ids']\n",
    "    labels=batch['labels']\n",
    "    attention_mask=batch['attention_mask']\n",
    "    outputs=model(input_ids,attention_mask=attention_mask)\n",
    "    loss=loss_fn(outputs.view(-1,outputs.size(-1)),labels.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "def evaluate(model,dataloader,loss_fn):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "    total_count=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids=batch['input_ids']\n",
    "            labels=batch['labels']\n",
    "            attention_mask=['attention_mask']\n",
    "            outputs=model(input_ids,attention_mask=attention_mask)\n",
    "            loss=loss_fn(outputs.view(-1,outputs.size(-1),labels.view(-1)))\n",
    "            total_loss+=loss.item()*input_ids.size(0)\n",
    "            total_count+=input_ids.size(0)\n",
    "    return total_loss/total_count        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
