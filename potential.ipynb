{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(self, vocab_file):\n",
    "        with open(vocab_file, 'r') as f:\n",
    "            self.vocab = {w.strip(): i for i, w in enumerate(f)}\n",
    "        self.reverse_vocab = {i: w for w, i in self.vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        return torch.tensor([[self.vocab.get(word, self.vocab['<unk>']) for word in text.split()]])\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return ' '.join([self.reverse_vocab.get(id.item(), '<unk>') for id in token_ids[0]])\n",
    "\n",
    "tokenizer = CustomTokenizer('path_to_your_vocab_file.txt')\n",
    "\n",
    "def encode(text):\n",
    "    return tokenizer.encode(text)\n",
    "\n",
    "def decode(token_ids):\n",
    "    return tokenizer.decode(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Initialize the tokenizer\n",
    "# You'll need to train or load a tokenizer appropriate for your task\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"gpt2\")  # or your custom tokenizer\n",
    "\n",
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Encodes raw text into token IDs.\n",
    "    \"\"\"\n",
    "    return tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "def decode(token_ids):\n",
    "    \"\"\"\n",
    "    Decodes token IDs back into text.\n",
    "    \"\"\"\n",
    "    return tokenizer.decode(token_ids[0])\n",
    "\n",
    "# Update the main function to use these:\n",
    "\n",
    "def main():\n",
    "    # ... (previous code remains the same)\n",
    "\n",
    "    # Generate some text\n",
    "    prompt = \"Once upon a time\"\n",
    "    input_ids = encode(prompt)\n",
    "    input_ids = input_ids.to(device)\n",
    "    generated_ids = model.generate(input_ids, max_length=50)\n",
    "    generated_text = decode(generated_ids)\n",
    "    print(\"Generated text:\", generated_text)\n",
    "\n",
    "    # ... (rest of the function remains the same)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
